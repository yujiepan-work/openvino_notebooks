{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cacbe6b4",
   "metadata": {
    "id": "rQc-wXjqrEuR"
   },
   "source": [
    "# Accelerate Inference of sparse Transformer models with OpenVINO™ and 4th Gen Intel&reg; Xeon&reg; Scalable processors\n",
    "This tutorial demonstrates how to improve performance of sparse Transformer models with [OpenVINO](https://docs.openvino.ai/) on 4th Gen Intel® Xeon® Scalable processors. It uses a pre-trained model from the [HuggingFace Transformers](https://huggingface.co/transformers/) library and shows how to convert it to the OpenVINO™ IR format and run inference of the model on the CPU using a dedicated runtime option that enables sparsity optimizations. It also demonstrates how to get more performance stacking sparsity with 8-bit quantization. To simplify the user experience, the [HuggingFace Optimum](https://huggingface.co/docs/optimum) library is used to convert the model to the OpenVINO™ IR format and quantize it using [Neural Network Compression Framework](https://github.com/openvinotoolkit/nncf). It consists of the following steps:\n",
    "\n",
    "- Install prerequisites\n",
    "- Download and quantize sparse BERT model from the public using HuggingFace Optimum for OpenVINO.\n",
    "- Compare sparse 8-bit vs. dense 8-bit inference performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bef22e9",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc9afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optimum[openvino]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import nncf  # noqa: F401\n",
    "except ImportError:\n",
    "    !pip install git+https://github.com/openvinotoolkit/nncf.git#egg=nncf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b41e6-132b-40da-b3b9-91bacba29e31",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771388d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from optimum.intel.openvino import OVQuantizer\n",
    "from optimum.intel.openvino import OVConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603a481",
   "metadata": {},
   "source": [
    "## Quantize model with HuggingFace Optimum API\n",
    "The sparsity acceleration MatMul operations is available only in the case when these operations are quantized into 8-bit precision. If the model is not quantized it can be done using either way availble for OpenVINO models. For more details refer [here](https://docs.openvino.ai/latest/openvino_docs_model_optimization_guide.html). In this tutorial we use the HuggingFace Optimum API to quantize the model. The HuggingFace Optimum API is a high-level API that allows to convert and quantize models from the HuggingFace Transformers library to the OpenVINO™ IR format. For more details refer to the [HuggingFace Optimum documentation](https://huggingface.co/docs/optimum/intel/optimization_ov)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"yujiepan/bert-base-uncased-sst2-unstructured-sparsity-80\"\n",
    "quantized_sparse_dir = Path(\"bert_80_sparse_quantized\")\n",
    "\n",
    "# Instantiate model and tokenizer in PyTorch and load them from the HF Hub\n",
    "torch_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Define a function that tokenizes the data and returns it in the format expected by the model.\n",
    "    \n",
    "    :param: examples: a dictionary containing the input data which are the items from caliration dataset.\n",
    "            tokenizer: a tokenizer object that is used to tokenize the text data.\n",
    "    :returns:\n",
    "            the data that can be fed directly to the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"], padding=\"max_length\", max_length=128, truncation=True\n",
    "    )\n",
    "\n",
    "# Create quantization config (default) and OVQuantizer\n",
    "# OVConfig is a wrapper class on top of NNCF config. \n",
    "# Use \"compression\" field to control quantization parameters\n",
    "# For more information about the parameters refer to NNCF GitHub documentatioin\n",
    "quantization_config = OVConfig()\n",
    "quantizer = OVQuantizer.from_pretrained(torch_model, feature=\"sequence-classification\")\n",
    "\n",
    "# Instantiate a dataset and convert it to calibration dataset using HF API\n",
    "# The latter one produces a model input\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "calibration_dataset = quantizer.get_calibration_dataset(\n",
    "    \"glue\",\n",
    "    dataset_config_name=\"sst2\",\n",
    "    preprocess_function=partial(preprocess_function, tokenizer=tokenizer),\n",
    "    num_samples=100,\n",
    "    dataset_split=\"train\",\n",
    ")\n",
    "# Apply static quantization and export the resulting quantized model to OpenVINO IR format\n",
    "quantizer.quantize(\n",
    "    quantization_config=quantization_config, calibration_dataset=calibration_dataset, save_directory=quantized_sparse_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6830eb7",
   "metadata": {},
   "source": [
    "## Benchmark quantized dense inference performance\n",
    "Benchmark dense inference performance using parallel execution on four CPU cores to simulate a small instance in the cloud infrastructure. Sequense length is set to 16 which is common for multiple use cases, e.g. conversational AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa895f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump benchmarking config for dense inference\n",
    "with open(\"perf_config.json\", \"w\") as outfile:\n",
    "    outfile.write(\n",
    "\"\"\"\n",
    "{\n",
    "    \"CPU\": {\"NUM_STREAMS\": 4, \"INFERENCE_NUM_THREADS\": 4}\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c7526",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m $quantized_sparse_dir/openvino_model.xml -shape \"input_ids[1,16],attention_mask[1,16],token_type_ids[1,16]\" -load_config perf_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9151b11",
   "metadata": {},
   "source": [
    "## Benchmark quantized sparse inference performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad77ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump benchmarking config for dense inference\n",
    "# \"CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE\" controls minimum sparsity rate for weights to consider \n",
    "# for sparse optimization at the runtime.\n",
    "with open(\"perf_config_sparse.json\", \"w\") as outfile:\n",
    "    outfile.write(\n",
    "\"\"\"\n",
    "{\n",
    "    \"CPU\": {\"NUM_STREAMS\": 4, \"INFERENCE_NUM_THREADS\": 4, \"CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE\": 0.8}\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddd8b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m $quantized_sparse_dir/openvino_model.xml -shape \"input_ids[1,16],attention_mask[1,16],token_type_ids[1,16]\" -load_config perf_config_sparse.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d4d61",
   "metadata": {},
   "source": [
    "## When this might be helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135c8526",
   "metadata": {},
   "source": [
    "This feauture can improve inference performance for models with sparse weights in the scenarios when the model is deployed to handle multiple requests in parallel asyncronously. It is especially helpful in the case of small sequence length, e.g. 32 and lower.\n",
    "\n",
    "For more details about the asynchronous inference with OpenVINO refer to the following documentation:\n",
    "- [Deployment Optimization Guide](https://docs.openvino.ai/latest/openvino_docs_deployment_optimization_guide_common.html#doxid-openvino-docs-deployment-optimization-guide-common-1async-api)\n",
    "- [Inference Request API](https://docs.openvino.ai/latest/openvino_docs_OV_UG_Infer_request.html#doxid-openvino-docs-o-v-u-g-infer-request-1in-out-tensors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ov_notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "25fad543ad216e1bba3a2237c544f0cea27ae49559091fac659fb84d6cd5fdd7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
