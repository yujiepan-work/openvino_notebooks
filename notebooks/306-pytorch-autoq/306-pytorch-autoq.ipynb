{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoQ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Settings\n",
    "\n",
    "Import NNCF and all auxiliary packages from your Python code.\n",
    "\n",
    "> **NOTE**: All NNCF logging messages below ERROR level (INFO and WARNING) are disabled to simplify the tutorial. For production use, it is recommended to enable logging by removing `set_log_level(logging.ERROR)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nvme2/yujiepan/workspace/jpqd-clean/nncf/nncf/torch/dynamic_graph/patch_pytorch.py:163: UserWarning: Not patching unique_dim since it is missing in this version of PyTorch\n",
      "  warnings.warn(\"Not patching {} since it is missing in this version of PyTorch\".format(op_name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import nncf  # Important - should be imported directly after torch.\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from nncf.common.utils.logger import set_log_level\n",
    "set_log_level(logging.ERROR)  # Disables all NNCF info and warning messages.\n",
    "\n",
    "from nncf import NNCFConfig\n",
    "from nncf.torch import register_default_init_args\n",
    "from nncf.torch import create_compressed_model\n",
    "\n",
    "from openvino.runtime import Core\n",
    "\n",
    "from model_utils import MobileNetV2\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using %s device.\" % device)\n",
    "\n",
    "MODEL_DIR = Path(\"model\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "DATA_DIR = Path(\"data\")\n",
    "BASE_MODEL_NAME = \"mobilenet-V2\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Path for the pretrained fp32 model weights\n",
    "fp32_pth_path = Path(MODEL_DIR / (BASE_MODEL_NAME + \"_fp32\")).with_suffix(\".pth\")\n",
    "fp32_onnx_path = fp32_pth_path.with_suffix(\".onnx\")\n",
    "quantized_pth_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + '_quantized')).with_suffix(\".pth\")\n",
    "quantized_onnx_path = quantized_pth_path.with_suffix(\".onnx\")\n",
    "if not fp32_pth_path.exists():\n",
    "    fp32_pth_url = \"http://hsw1.jf.intel.com/share/bootstrapNAS/checkpoints/cifar10/mobilenet_v2.pt\"\n",
    "    download_file(fp32_pth_url, directory=MODEL_DIR, filename=fp32_pth_path.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = DATA_DIR / \"cifar10\"\n",
    "\n",
    "image_size = 32\n",
    "normalize = transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2471, 0.2435, 0.2616))\n",
    "val_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(image_size, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(DATASET_DIR, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(DATASET_DIR, train=False, transform=val_transform, download=False)\n",
    "\n",
    "batch_size = 64\n",
    "batch_size_val = 2000\n",
    "workers = 4\n",
    "pin_memory = device != 'cpu'\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size_val, shuffle=False,\n",
    "                        num_workers=workers, pin_memory=pin_memory, drop_last=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=workers, pin_memory=pin_memory, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=\":f\"):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print(\"\\t\".join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
    "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, verbose=False):\n",
    "    batch_time = AverageMeter(\"Time\", \":3.3f\")\n",
    "    losses = AverageMeter(\"Loss\", \":2.3f\")\n",
    "    top1 = AverageMeter(\"Acc@1\", \":2.2f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \":2.2f\")\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            losses.update(loss.item(), images.size(0))\n",
    "            top1.update(acc1[0], images.size(0))\n",
    "            top5.update(acc5[0], images.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "        if verbose:\n",
    "            print(\" * Test Loss {losses.avg:.3f} Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\".format(losses=losses, top1=top1, top5=top5))\n",
    "    return top1.avg, top5.avg, losses.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, epoch, compression_ctrl):\n",
    "    batch_time = AverageMeter(\"Time\", \":3.3f\")\n",
    "    losses = AverageMeter(\"Loss\", \":2.3f\")\n",
    "    top1 = AverageMeter(\"Acc@1\", \":2.2f\")\n",
    "    top5 = AverageMeter(\"Acc@5\", \":2.2f\")\n",
    "    progress = ProgressMeter(\n",
    "        len(train_loader), [batch_time, losses, top1, top5], prefix=\"Epoch:[{}]\".format(epoch)\n",
    "    )\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    compression_scheduler = compression_ctrl.scheduler\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, target) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(images)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), images.size(0))\n",
    "        top1.update(acc1[0], images.size(0))\n",
    "        top5.update(acc5[0], images.size(0))\n",
    "\n",
    "        # compute gradient and do opt step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        print_frequency = 50\n",
    "        if i % print_frequency == 0 or i == len(train_loader) - 1:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Float32 Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 model accuracy: acc1: 93.91 acc5: 99.83\n",
      "FP32 ONNX model was exported to model/mobilenet-V2_fp32.onnx.\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2()\n",
    "model.load_state_dict(torch.load(fp32_pth_path, map_location=\"cpu\"))\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "acc1, acc5, _ = validate(model, val_loader, criterion) \n",
    "print('FP32 model accuracy: acc1: %.2f acc5: %.2f' % (acc1, acc5))\n",
    "\n",
    "dummy_input = torch.randn(1, 3, image_size, image_size).to(device)\n",
    "torch.onnx.export(model, dummy_input, fp32_onnx_path)\n",
    "print(\"FP32 ONNX model was exported to %s.\" % fp32_onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start AutoQ.\n",
      "Trial evaluation acc1: 93.91.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 10.05.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 9.84.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 10.02.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 9.81.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 40.74.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 10.00.\n",
      "Trial evaluation acc1: 32.01.\n",
      "Statistics of the quantization algorithm:\n",
      "+--------------------------------+-------+\n",
      "|        Statistic's name        | Value |\n",
      "+================================+=======+\n",
      "| Ratio of enabled quantizations | 100   |\n",
      "+--------------------------------+-------+\n",
      "\n",
      "Statistics of the quantization share:\n",
      "+----------------------------------+--------------------+\n",
      "|         Statistic's name         |       Value        |\n",
      "+==================================+====================+\n",
      "| Symmetric WQs / All placed WQs   | 100.00 % (53 / 53) |\n",
      "+----------------------------------+--------------------+\n",
      "| Asymmetric WQs / All placed WQs  | 0.00 % (0 / 53)    |\n",
      "+----------------------------------+--------------------+\n",
      "| Signed WQs / All placed WQs      | 100.00 % (53 / 53) |\n",
      "+----------------------------------+--------------------+\n",
      "| Unsigned WQs / All placed WQs    | 0.00 % (0 / 53)    |\n",
      "+----------------------------------+--------------------+\n",
      "| Per-tensor WQs / All placed WQs  | 0.00 % (0 / 53)    |\n",
      "+----------------------------------+--------------------+\n",
      "| Per-channel WQs / All placed WQs | 100.00 % (53 / 53) |\n",
      "+----------------------------------+--------------------+\n",
      "| Placed WQs / Potential WQs       | 50.48 % (53 / 105) |\n",
      "+----------------------------------+--------------------+\n",
      "| Symmetric AQs / All placed AQs   | 81.48 % (44 / 54)  |\n",
      "+----------------------------------+--------------------+\n",
      "| Asymmetric AQs / All placed AQs  | 18.52 % (10 / 54)  |\n",
      "+----------------------------------+--------------------+\n",
      "| Signed AQs / All placed AQs      | 40.74 % (22 / 54)  |\n",
      "+----------------------------------+--------------------+\n",
      "| Unsigned AQs / All placed AQs    | 59.26 % (32 / 54)  |\n",
      "+----------------------------------+--------------------+\n",
      "| Per-tensor AQs / All placed AQs  | 66.67 % (36 / 54)  |\n",
      "+----------------------------------+--------------------+\n",
      "| Per-channel AQs / All placed AQs | 33.33 % (18 / 54)  |\n",
      "+----------------------------------+--------------------+\n",
      "\n",
      "Statistics of the bitwidth distribution:\n",
      "+--------------+---------------------+--------------------+--------------------+\n",
      "| Num bits (N) | N-bits WQs / Placed |    N-bits AQs /    | N-bits Qs / Placed |\n",
      "|              |         WQs         |     Placed AQs     |         Qs         |\n",
      "+==============+=====================+====================+====================+\n",
      "| 8            | 39.62 % (21 / 53)   | 51.85 % (28 / 54)  | 45.79 % (49 / 107) |\n",
      "+--------------+---------------------+--------------------+--------------------+\n",
      "| 4            | 58.49 % (31 / 53)   | 48.15 % (26 / 54)  | 53.27 % (57 / 107) |\n",
      "+--------------+---------------------+--------------------+--------------------+\n",
      "| 2            | 1.89 % (1 / 53)     | 0.00 % (0 / 54)    | 0.93 % (1 / 107)   |\n",
      "+--------------+---------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\") # Avoid warnings in torchvision and pandas.\n",
    "\n",
    "autoq_iter_number = 20 # The number of search episodes by AutoQ.\n",
    "autoq_compression_ratio = 0.20 # Target quantized model size relative to FP32 model, 0.25 for uniform int8 quantization and 0.125 for uniform int4.\n",
    "eval_subset_ratio = 1.0 # Evaluation ratio of the subset for AutoQ.\n",
    "\n",
    "config = {\n",
    "    \"model\": BASE_MODEL_NAME,\n",
    "    \"dataset\": \"cifar10\",\n",
    "    \"input_info\": {\n",
    "        \"sample_size\": [1, 3, image_size, image_size]\n",
    "    },\n",
    "    \"target_device\": \"VPU\",\n",
    "    \"compression\": {\n",
    "        \"algorithm\": \"quantization\",\n",
    "        \"initializer\": {\n",
    "            \"batchnorm_adaptation\": {\n",
    "                \"num_bn_adaptation_samples\": 512\n",
    "            },\n",
    "            \"range\": {\n",
    "                \"type\": \"mean_min_max\",\n",
    "                \"num_init_samples\": 512\n",
    "            },\n",
    "            \"precision\": {\n",
    "                \"type\": \"autoq\",\n",
    "                \"bits\": [2, 4, 8],\n",
    "                \"iter_number\": autoq_iter_number,\n",
    "                \"compression_ratio\": autoq_compression_ratio,\n",
    "                \"eval_subset_ratio\": eval_subset_ratio,\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# AutoQ evaluation function to decide the best policy.\n",
    "def autoq_eval_fn(model, eval_loader):\n",
    "    acc1, acc5, _ = validate(model, eval_loader, criterion)\n",
    "    print('Trial evaluation acc1: %.2f.' % acc1)\n",
    "    return acc1\n",
    "\n",
    "\n",
    "nncf_config = NNCFConfig.from_dict(config)\n",
    "nncf_config = register_default_init_args(\n",
    "    nncf_config,\n",
    "    train_loader=train_loader,\n",
    "    criterion=criterion,\n",
    "    val_loader=val_loader,\n",
    "    autoq_eval_fn=autoq_eval_fn,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print('Start AutoQ.')\n",
    "compression_ctrl, model = create_compressed_model(model, nncf_config)\n",
    "print(compression_ctrl.statistics().to_str())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the Quantized Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[0][  0/781]\tTime 0.566 (0.566)\tLoss 0.895 (0.895)\tAcc@1 71.88 (71.88)\tAcc@5 96.88 (96.88)\n",
      "Epoch:[0][ 50/781]\tTime 0.139 (0.162)\tLoss 0.738 (0.749)\tAcc@1 76.56 (74.57)\tAcc@5 100.00 (98.19)\n",
      "Epoch:[0][100/781]\tTime 0.235 (0.160)\tLoss 0.655 (0.733)\tAcc@1 76.56 (74.68)\tAcc@5 98.44 (98.33)\n",
      "Epoch:[0][150/781]\tTime 0.131 (0.162)\tLoss 0.578 (0.726)\tAcc@1 81.25 (75.19)\tAcc@5 98.44 (98.46)\n",
      "Epoch:[0][200/781]\tTime 0.129 (0.156)\tLoss 0.609 (0.716)\tAcc@1 79.69 (75.67)\tAcc@5 98.44 (98.47)\n",
      "Epoch:[0][250/781]\tTime 0.133 (0.155)\tLoss 0.688 (0.707)\tAcc@1 75.00 (75.89)\tAcc@5 100.00 (98.46)\n",
      "Epoch:[0][300/781]\tTime 0.170 (0.157)\tLoss 0.624 (0.695)\tAcc@1 79.69 (76.37)\tAcc@5 100.00 (98.54)\n",
      "Epoch:[0][350/781]\tTime 0.131 (0.157)\tLoss 0.841 (0.690)\tAcc@1 73.44 (76.62)\tAcc@5 98.44 (98.58)\n",
      "Epoch:[0][400/781]\tTime 0.132 (0.155)\tLoss 0.702 (0.683)\tAcc@1 76.56 (76.77)\tAcc@5 100.00 (98.63)\n",
      "Epoch:[0][450/781]\tTime 0.147 (0.154)\tLoss 0.661 (0.678)\tAcc@1 76.56 (76.82)\tAcc@5 100.00 (98.65)\n",
      "Epoch:[0][500/781]\tTime 0.207 (0.155)\tLoss 0.427 (0.673)\tAcc@1 84.38 (77.05)\tAcc@5 100.00 (98.65)\n",
      "Epoch:[0][550/781]\tTime 0.132 (0.158)\tLoss 0.431 (0.667)\tAcc@1 89.06 (77.25)\tAcc@5 100.00 (98.67)\n",
      "Epoch:[0][600/781]\tTime 0.173 (0.156)\tLoss 0.581 (0.662)\tAcc@1 82.81 (77.41)\tAcc@5 98.44 (98.70)\n",
      "Epoch:[0][650/781]\tTime 0.160 (0.155)\tLoss 0.464 (0.660)\tAcc@1 84.38 (77.52)\tAcc@5 100.00 (98.71)\n",
      "Epoch:[0][700/781]\tTime 0.145 (0.154)\tLoss 0.568 (0.656)\tAcc@1 81.25 (77.65)\tAcc@5 96.88 (98.73)\n",
      "Epoch:[0][750/781]\tTime 0.125 (0.154)\tLoss 0.764 (0.652)\tAcc@1 76.56 (77.83)\tAcc@5 98.44 (98.74)\n",
      "Epoch:[0][780/781]\tTime 0.125 (0.153)\tLoss 0.409 (0.650)\tAcc@1 84.38 (77.90)\tAcc@5 100.00 (98.74)\n",
      "Epoch 0, Test acc1: 81.68 acc5: 99.19\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Start training\n",
    "best_acc1 = 0.0\n",
    "for epoch in range(n_epochs):    \n",
    "    # Train for one epoch\n",
    "    train_epoch(model, train_loader, criterion, optimizer, epoch, compression_ctrl)\n",
    "\n",
    "    # Validation\n",
    "    acc1, acc5, _ = validate(model, val_loader, criterion) \n",
    "    print('Epoch %d, Test acc1: %.2f acc5: %.2f' % (epoch, acc1, acc5))\n",
    "\n",
    "    if acc1 > best_acc1:\n",
    "        torch.save(model.state_dict(), quantized_pth_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Quantized Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized ONNX model exported to output/mobilenet-V2_quantized.onnx.\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(quantized_pth_path, map_location='cpu'))\n",
    "compression_ctrl.export_model(quantized_onnx_path)\n",
    "print(f\"Quantized ONNX model exported to {quantized_onnx_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the fp32 and quantized model to OpenVINO Intermediate Representation (IR). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_ir_path = fp32_onnx_path.with_suffix('.xml')\n",
    "quantized_ir_path = quantized_onnx_path.with_suffix('.xml')\n",
    "\n",
    "if not fp32_ir_path.exists():\n",
    "    !mo --input_model $fp32_onnx_path --output_dir $MODEL_DIR\n",
    "\n",
    "if not quantized_ir_path.exists():\n",
    "    !mo --input_model $quantized_onnx_path --output_dir $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the inference performance of the `FP32` and `quantized` models, using [Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html) - inference performance measurement tool in OpenVINO.\n",
    "\n",
    "> **NOTE**: This notebook runs `benchmark_app` for 15 seconds to give a quick indication of performance. For more accurate performance, please see `benchmark_app` document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark FP32 model (IR)\n",
      "Count:          88968 iterations\n",
      "Duration:       15002.47 ms\n",
      "Latency:\n",
      "Throughput: 5930.22 FPS\n",
      "Benchmark quantized model (IR)\n",
      "Count:          73208 iterations\n",
      "Duration:       15001.85 ms\n",
      "Latency:\n",
      "Throughput: 4879.93 FPS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_benchmark_output(benchmark_output):\n",
    "    parsed_output = [line for line in benchmark_output if not (line.startswith(r\"[\") or line.startswith(\"  \") or line == \"\")]\n",
    "    print(*parsed_output, sep='\\n')\n",
    "\n",
    "\n",
    "print('Benchmark FP32 model (IR)')\n",
    "benchmark_output = ! benchmark_app -m $fp32_ir_path -d CPU -api async -t 15\n",
    "parse_benchmark_output(benchmark_output)\n",
    "\n",
    "print('Benchmark quantized model (IR)')\n",
    "benchmark_output = ! benchmark_app -m $quantized_ir_path -d CPU -api async -t 15\n",
    "parse_benchmark_output(benchmark_output)\n",
    "\n",
    "ie = Core()\n",
    "ie.get_property(\"CPU\", \"FULL_DEVICE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('jpqd-clean')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cde1cd94eeca7d2f2787db0ccec782199674920a9b8d17d7a339643b6ae9fa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
